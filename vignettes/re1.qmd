---
title: "re1"
format: html
editor: visual
---

# Setup

```{r}
library(geosphere)
library(phenocamr)
library(here)
library(GenSA)
library(dplyr)
library(readr)
library(lubridate)
library(terra)
library(MODISTools)
library(ggplot2)
library(tidyterra)
library(patchwork)
library(appeears)
library(keyring)

if (!dir.exists(here("data"))) {
  dir.create(here("data"))
}
```

```{r}
# Download the data
out_dir <- here("data")

if (!file.exists(out_dir)) {
  download.file(
    phenocamr::download_phenocam(
      site = "harvard$",      
      veg_type = "DB",        # Deciduous broadleaf vegetation
      roi_id = "1000",        # region ID
      daymet = TRUE,          # Daymet weather data 
      phenophase = TRUE,      # Phenophase dates
      trim = 2022,            
    )
  )
}
```

```{r}
# Read the data. We use "#" to skip the metadata lines at the top of the file
harvard_phenocam_data <- read_csv(
  here("data", "harvard_DB_1000_3day.csv"),
  comment = "#"
)

# Create 'harvard_temp' (calculate Tmean)
harvard_temp <- harvard_phenocam_data %>%
  mutate(date = as.Date(date)) %>%
  mutate(year = as.numeric(format(date, "%Y"))) %>%
  group_by(year) %>%
  mutate(
    tmean = (tmax..deg.c. + tmin..deg.c.) / 2
  ) %>%
  mutate(
    gdd = cumsum(ifelse(tmean >= 5, tmean - 5, 0))
  ) %>%
  ungroup() %>%
  select(date, year, tmean, gdd)

# Check the result
head(harvard_temp)
```

```{r}
# Read the data
harvard_phenocam_data_transition <- read_csv(
  here("data", "harvard_DB_1000_3day_transition_dates.csv"),
  comment = "#"
)

harvard_phenology <- harvard_phenocam_data_transition %>%
  # Filter for the start of spring (rising greenness)
  filter(direction == "rising", gcc_value == "gcc_90") %>%
  
  # Create the 'year' and 'doy' columns from the date
  mutate(
    trans_date = as.Date(transition_25),         # Convert text to date
    year = as.numeric(format(trans_date, "%Y")), # Extract year 
    doy = as.numeric(format(trans_date, "%j"))   # Extract day of year 
  ) %>%
  
  # Select only what we need for the model
  select(year, doy, transition_25, threshold_25)

# Check the result 
print(head(harvard_phenology))
```

# How to improve the model

The model used in chapter 6 is a simple Growing Degree Day (GDD) model. It assumes that temperature is the only driver of spring leaf-out. Once the daily temperature exceeds a base threshold (Tbase), the plant accumulates "heat units." When the bucket of heat units is full (F∗), leaves appear.

## Dormancy

Vegetation have a sleep mode called dormancy to protect themselves from winter. This prevent them to respond to warmth until they have experienced enough cold weather (chilling) to wake up. Including this dormancy parameter prevents the model from predicting leaf-out during warm days in early winter. The heat budget is locked until the cold budget is full.

In order to apply this to the model, we could add a a rule where GDD accumulation only starts after a certain number of cold days have passed.

## Soil temperature

The current model uses air temperature but vegetation have roots in the ground which is warming up slower that than air. Roots need to be active to pump water up to the buds before the leaves can expand. If the soil is still frozen, the roots can't function, even if the air is warm.

The model could be improved by also estimating soil temperature considering that it is often a lagged and damped version of air temperature.

## Photoperiod

Temperatures varies greatly year to year but the length of the day is perfectly consistent and vegetation usually use day length like a calendar. It helps the plant distinguish between a warm day in February (short days) and a warm day in May (long days) for example.

We can implement this by modifying the critical threshold (F\*). Plants need less heat to leaf out if the days are very long (late spring), but more heat if the days are short (early spring).

# The basic GGD model

```{r}
gdd_model <- function(par, temp) {
  
  # We unpack the parameters (for readability)
  t_base <- par[1]  # The base temperature (like 5°C)
  f_crit <- par[2]  # The critical heat threshold (like 300 GDD)
  
  # We calculate daily heat units (forcing). If temp > t_base, we accumulate heat. If colder, we accumulate 0.
  # We use pmax() which is a faster version of ifelse() for vectors.
  daily_forcing <- pmax(0, temp - t_base)
  
  # We calculate the state of forcing (cumulative sum). This creates a running total of heat for every day of the year.
  accumulated_forcing <- cumsum(daily_forcing)
  
  # We find the trigger day. Leaf-out happens when accumulated heat >= treshold 
  # We take the first one ([1]) using which().
  doy <- which(accumulated_forcing >= f_crit)[1]
  
  return(doy)
}
```

# Adding photoperiod

Now we want to improve the model by adding the photoperiod. The concept is that if the days are long (late spring), trees lower their defense mechanisms and need less heat to leaf out.

To implement this, we need two things:

-   Data: We need to know the day length (in hours) for every day of the year.

-   Math: We need a formula where longer days make the threshold (Fcrit) smaller.

## Photoperiod GDD model

The length of the day depends on latitude and the day of the year. We can use the geosphere package for this.

```{r}
# The Photoperiod GDD Model
model_photoperiod <- function(par, temp, latitude) {
  
  # We unpack the parameters. Now we have 3 parameters to optimize
  t_base <- par[1]      # Base temperature
  f_crit <- par[2]      # Heat treshold 
  k_photo <- par[3]     # Photoperiod sensitivity
  
  # We calculate daily heat units
  daily_forcing <- pmax(0, temp - t_base)
  
  # We calculate accumulated heat 
  accumulated_forcing <- cumsum(daily_forcing)
  
  # We calculate the day length for every day of the year (length of temp vector)
  doy_vector <- 1:length(temp) 
  day_len <- geosphere::daylength(lat = latitude, doy = doy_vector)
  
  # We define the Variable threshold. The threshold changes every day. Longer days = lower threshold.
  daily_threshold <- f_crit - (k_photo * day_len)
  
  # We finally find the trigger day.
  doy <- which(accumulated_forcing >= daily_threshold)[1]
  
  return(doy)
}
```

## The cost function

This function measures how well the model predictions match observation of leaf-out dates. A lower RMSE means a better model fit.

```{r}

harvard_latitude <- 42.536669726040884

# We define the cost function
rmse_photoperiod <- function(par, data) {
  
  # drivers = temp, validation = observed dates
  drivers <- data$drivers
  validation <- data$validation
  
  # We run the model for every year in the driver data. This creates a dataframe of "year" and "pred" (predicted doy)
  predictions <- drivers %>%
    group_by(year) %>%
    summarise(
      pred = model_photoperiod(
        par = par, 
        temp = tmean,  
        latitude = harvard_latitude
      )
    )
  
  # We compare to reality by joining predictions with the 'validation' data (observed 'doy')
  comparison <- left_join(predictions, validation, by = "year")
  
  # We calculate the RMSE 
  rmse <- sqrt(mean((comparison$pred - comparison$doy)^2, na.rm = TRUE))
  
  return(rmse)
}
```

## Optimization of the RMSE

```{r}
# We set up the data for the function
data_for_optim <- list(
  drivers = harvard_temp,
  validation = harvard_phenology
)

# We define the lower and upper boundaries of the 3 parameters
#                 T_base,  F_crit,  k_photo
lower_bounds <- c(   0,      50,       0)
upper_bounds <- c(   10,     500,      20)

# First guess
start_par <- c(0, 200, 5) 

# We run the optimization
optim_result <- GenSA(
  par = start_par,
  fn = rmse_photoperiod,    # Our cost function
  lower = lower_bounds,
  upper = upper_bounds,
  control = list(max.call = 4000), # Limit it to 4000 tries
  data = data_for_optim     
)

# We extract the best parameters
best_pars <- optim_result$par

cat("Optimal parameters:\n")
cat("T_base:", round(best_pars[1], 2), "\n")
cat("F_crit:", round(best_pars[2], 2), "\n")
cat("k_photo:", round(best_pars[3], 2), "\n")
cat("Final RMSE:", round(optim_result$value, 3), "\n")
```

# Spatial scaling

Now that we have the optimal parameters we need to apply this model to a map of temperature to observe the predicted leaf-out dates across the New England region. In order to do this we need to download the gridded daily temperature for the region and then run the `model_photoperiod` function on every pixel of that map.

```{r}
# We download the temperature raster data
dest_file <- here("data", "daymet_2012_subset.nc")

if (!file.exists(dest_file)) {
  download.file(
    url = "https://github.com/fabern/handfull_of_pixels/raw/refs/heads/main/data/DAYMET.004_2012/DAYMET.004_1km_aid0001.nc",
    destfile = dest_file,
    mode = "wb"
  )
}

# We load and prepare the data
r_daymet <- terra::rast(dest_file)

# We set the coordinate reference system
terra::crs(r_daymet) <- "epsg:4326"

# We calculate the Tmean for the map
tmean_map <- (r_daymet["tmax"] + r_daymet["tmin"]) / 2

# We limit to the first 180 days to save processing time because spring phenology happens in the first half of the year
tmean_map <- subset(tmean_map, 1:180)

# We create a latitude raster using terra::init(). When we use "y" as the second argument, it fills each cell with its y-coordinate (which is latitude in our case).
lat_raster <- terra::init(tmean_map[[1]], "y")

# We combine the 2 rasters (temperature and latitude) into one stack of 181 layers in total.
#   - Layers 1 to 180: temperature for each day
#   - Layer 181: latitude
input_stack <- c(tmean_map, lat_raster)
```

```{r}
# Now we we define a function to apply to each pixel that terra:app() can use that will receive the vector "x" of length 181. 
# The function will:
#   1. Extract temperature (first 180 values)
#   2. Extract latitude (last value)
#   3. Run the model
#   4. Return the prediction

apply_model_to_pixel <- function(x, par) {
  
  # We define the number of days (181-1 = 180) for each latitude
  n_days <- length(x) - 1
  
  # We split the vector:
  # - Temperature: positions 1 to 180
  # - Latitude: position 181
  temp <- x[1:n_days]
  lat <- x[n_days + 1]
  
  # We filter missing data
  if (any(is.na(temp)) || is.na(lat)) {
    return(NA)
  }
  
  # Run the photoperiod model with each pixel using the correct latitude
  doy <- model_photoperiod(
    par = par,
    temp = temp,
    latitude = lat
  )
  
  return(doy)
}
```

```{r}
# We apply the model to every pixel using terra:app()
predicted_map <- terra::app(
  x = input_stack,
  fun = apply_model_to_pixel,
  par = best_pars  # We use the optimized parameters from the optimzation step           
)

# Check results
cat("\nResults:\n")
cat("  - Min predicted DOY:", round(minmax(predicted_map)[1]), "\n")
cat("  - Max predicted DOY:", round(minmax(predicted_map)[2]), "\n")
```

```{r}
# visualization 
plot(
  predicted_map,
  main = " 2012 predicted leaf-out date photoperiod GDD model (day of year)",
  range = c(100, 150)
)
```

# Qualitative validation

For the qualitative comparison with the 2012 map of predicted result from our photoperiod model, we used the MODIS MCD12Q2 land cover dynamics product, developed by Boston University. We specifically analyzed the 'Greenup_1' band, which indicates the date of the start of the first vegetation cycle, defined as the point where EVI2 crosses 15% of the seasonal amplitude. The raw data provided in 'days since 1970-01-01' was converted to Day of Year (DOY) for comparison with our model. Fill values (32767) representing water or barren land were filtered out.

```{r}
# We define 3 tiles to cover the full latitude range because the API only allows 100x100km max tile size and we define a list to loop through them.
tiles <- list(
  north  = list(lat = 44.0, lon = -71.0, file = here("data", "modis_north.csv")),
  middle = list(lat = 43.0, lon = -71.0, file = here("data", "modis_middle.csv")),
  south  = list(lat = 42.0, lon = -71.0, file = here("data", "modis_south.csv"))
)

# We iterate through the list. For every tile, we check if the file already exists.
for (tile_name in names(tiles)) {
  
  tile <- tiles[[tile_name]]
  
  if (!file.exists(tile$file)) {
    message(paste("Downloading tile:", tile_name))
    
    subset_data <- MODISTools::mt_subset(
      product = "MCD12Q2",
      band = "Greenup.Num_Modes_01", 
      lat = tile$lat,
      lon = tile$lon,
      start = "2012-01-01",
      end = "2012-12-31",
      km_lr = 100, 
      km_ab = 100, 
      site_name = "NewEngland",
      internal = TRUE,
      progress = FALSE
    )
    write_csv(subset_data, tile$file)
  }
}
```

```{r}
# We create a function to use it for every CSV file
process_modis_tile <- function(file_path) {
  
  # Read the CSV
  raw <- read_csv(file_path, show_col_types = FALSE)
  df  <- as.data.frame(raw) # Convert to standard data.frame for MODISTools
  
  # Data cleaning : we filter fill Values (32767 indicates no data)
  df$value <- ifelse(df$value > 32766, NA, df$value)
  
  # Date Conversion: The raw data is days since 1970-01-01. So we convert this to a date object, then extract the day of year (DOY)
  date_vals <- as.Date("1970-01-01") + df$value
  df$value  <- as.numeric(format(date_vals, "%j"))
  
  # We convert to terra raster. We use reproject = FALSE here because we want to merge the tiles in their native projection first to avoid artifacts.
  terra_rast <- MODISTools::mt_to_terra(df, reproject = FALSE)
  
  return(terra_rast)
}

# We process all three tiles using the function
r_north  <- process_modis_tile(tiles$north$file)
r_middle <- process_modis_tile(tiles$middle$file)
r_south  <- process_modis_tile(tiles$south$file)

# We merge the tiles using terra::merge which combines rasters. If they overlap, it takes the value of the first one.
r_modis_merged <- terra::merge(r_north, r_middle, r_south)

# Now that we have one map, we project it to WGS84 (Lat/Lon) so it matches the coordinate system of the photoperiod model.
r_modis_ll <- terra::project(r_modis_merged, "epsg:4326")
```

```{r}
# We ensure that the photoperiod model has the correct CRS
terra::crs(predicted_map) <- "epsg:4326"

# We crop MODIS to fit the photoperiod model
r_modis_final <- terra::crop(r_modis_ll, predicted_map)

# Plotting

p_model <- ggplot() +
  geom_spatraster(data = predicted_map) +
  scale_fill_viridis_c(name = "DOY", limits = c(90, 150), na.value = "transparent") +
  labs(title = "Model prediction (Photoperiod)") +
  theme_bw()

p_modis <- ggplot() +
  geom_spatraster(data = r_modis_final) +
  scale_fill_viridis_c(name = "DOY", limits = c(90, 150), na.value = "transparent") +
  labs(title = "MODIS observation (MCD12Q2)") +
  theme_bw()

# Combine plots
p_model + p_modis
```

