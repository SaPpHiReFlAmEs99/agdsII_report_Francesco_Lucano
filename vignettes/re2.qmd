---
title: "Report 8.3: Land-cover"
author: Francesco Lucano
format: html
engine: knitr
editor: visual
warning: false
error: false
toc: true
---

# Introduction

Land-use and land-cover (LULC) maps aims to track land use its evolution. In order to improve classification accuracy, we are going to implement Supervised Machine Learning which improves on the general patterns identification offered by methods such as Unsupervised Clustering.

In this report, we develop a supervised classification workflow to map land cover using MODIS spectral data. We would like to improve the baseline model which utilizes a standard XGBoost (Extreme Gradient Boosting) algorithm trained on raw spectral bands.

# Proposed improvements

To increase the accuracy and robustness of the LULC model, we are going to implement the following 2 improvements:

## 1. Vegetation indices

The baseline model relies solely on raw reflectance bands. Even if machine learning algorithms can learns non-linear relationships, providing explicits features can accelerates the learning and improves the discrimination.

For this purpose, We will calculate and include NDVI (Normalized Difference Vegetation Index) and EVI (Enhanced Vegetation Index). It exploits the fundamental property of vegetation: chlorophyll absorbs visible red light for photosynthesis, while the cell structure of leaves strongly reflects near-infrared (NIR) light. It is highly effective at distinguishing vegetated areas (high values) from non-vegetated surfaces (low or negative values).However, it tends to saturate in very dense forests (high biomass), where the index saturates near 1.0 even as density increases.

The formula used is: $$NDVI = \frac{\rho_{NIR} - \rho_{Red}}{\rho_{NIR} + \rho_{Red}}$$

EVI is designed to optimize the vegetation signal in high biomass scenarios and reduce atmospheric effect. It includes the blue band to correct for aerosol scattering in the atmosphere and includes coefficients to adjust for the canopy background signal. Unlike NDVI, EVI does not saturate as easily in dense forests (like rain forests), making it superior for distinguishing between different forest types. But because it relies on the blue band (which is most sensitive to atmospheric scattering), it can be noisier if the atmospheric correction of the satellite data is imperfect.

The formula used is: $$EVI = G \times \frac{\rho_{NIR} - \rho_{Red}}{\rho_{NIR} + C_1 \times \rho_{Red} - C_2 \times \rho_{Blue} + L}$$

*Where* $G = 2.5$ (gain factor), $C_1 = 6$, $C_2 = 7.5$ (atmospheric resistance coefficients), and $L = 1$ (canopy background adjustment) following the MODIS-EVI algorithm (Huete et al., 2002).

## 2. Spatial cross-validation (spatial CV)

Standard K-fold cross-validation assumes that data points are independent. However, if we reference Tobler first law of geography, we have to consider that near things are more related than distant things, even if in the end everything is related to everything.

In this perspective, we will implement spatial cross-validation, where the training data is split into geographic clusters (blocks) rather than random subsets. Random splitting allows training points to sit immediately adjacent to test points, leading to "spatial leakage" and overly optimistic accuracy estimates. Spatial CV forces the model to predict on geographically distinct regions, ensuring that the tuned hyperparameters generalize truly to new, unseen locations.

# Additional improvements

We list 2 additional improvements that we aren't going to implement:

## 3. Algorithm selection (Random Forest vs. XGBoost)

Algorithms have their own strengths and their performance depends on the specific data structure and noise profile. So an improvement could come from comparing the performance of random forest against gradient boosting. Random forest is an ensemble of bagging trees that could be more robust to noise and usually requires less aggressive tuning than boosting algorithms. By testing a different algorithm, we can determine which mathematical approach best captures the decision boundaries of this specific landscape.

## 4. Removing correlated variables

Satellite spectral bands often exhibit high multicollinearity (for example the Blue and Green bands often contain very similar information). In order to improve this aspect, we could apply a correlation filter within the preprocessing recipe to remove highly correlated predictors. By reducing multicollinearity we simplify the model, we reduce computational cost, and we force the algorithm to use unique sources of information. This prevents the model from splitting importance across redundant variables, potentially leading to a more stable and easier to interpret model.
